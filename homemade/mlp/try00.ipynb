{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_sf,beds,bath,price,year_built,sqft,price_per_sqft,elevation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(492, 8)"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../sf_vs_nyc_house_data.csv', 'r') as f:\n",
    "    labels = f.readline()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "full_data = np.genfromtxt('../../sf_vs_nyc_house_data.csv', delimiter=',')[1:]\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train shape X: (393, 7) y: (99, 1) \n",
      "  Test shape X: (99, 7)  y: (99, 1) \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data[:,1:], full_data[:,:1], test_size=.2)\n",
    "\n",
    "print(f' Train shape X: {X_train.shape} y: {y_test.shape} \\n  Test shape X: {X_test.shape}  y: {y_test.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "NOW THAT WE HAVE THAT WE CAN START TO MAKE THE NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NN_torch(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(NN_torch, self).__init__()\n",
    "\n",
    "#         self.model = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(7,1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(1,1)\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, out_size) -> None:\n",
    "        self.l1 = np.random.rand(input_size,hidden_sizes[0])\n",
    "        self.b1 = np.zeros((1,hidden_sizes[0]))\n",
    "        self.l2 = np.random.rand(hidden_sizes[0],out_size)\n",
    "        self.b2 = np.zeros((1,out_size))\n",
    "\n",
    "    def activation_func(self, o, type):\n",
    "        if type.lower()== 'relu':\n",
    "            return np.where(o >= 0, o, 0)\n",
    "        if type.lower() == 'softmax':\n",
    "            #make val stable\n",
    "            o_stable = o - np.max(o, axis=1, keepdims=True)\n",
    "            o_exp = np.exp(o_stable) #e^x for each x in 0_stable\n",
    "            return o_exp / np.sum(o_exp, axis=1, keepdims=True)\n",
    "        if type.lower() == 'sigmoid':\n",
    "            return 1/(1+np.exp(-o))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.o1 = X @ self.l1 + self.b1\n",
    "        self.h1 = self.activation_func(self.o1, 'relu')\n",
    "        self.o2 = self.h1 @ self.l2 + self.b2\n",
    "        self.y_hat = self.activation_func(self.o2, 'sigmoid')\n",
    "        return self.y_hat\n",
    "    \n",
    "    def calc_loss(self, y_true, y_hat, type):\n",
    "        if type.lower() == 'mse':\n",
    "            return np.sum((y_true - y_hat) ** 2) / len(y_true)\n",
    "\n",
    "    def grad_relu(self, idx=None):\n",
    "        if idx is None:\n",
    "            return (self.h1 > 0).astype(float) #the as type flaot turn the bool mask in 0 or 1 which is what we wnat for the relu derivative\n",
    "        else:\n",
    "            return (self.h1[idx] > 0).astype(float)\n",
    "\n",
    "    def grad_mse(self, y_true): #return vextor (training_samps x 1)\n",
    "        return 2 * (self.y_hat - y_true) / len(y_true)\n",
    "    \n",
    "    def grad_sigmoid(self): #return vextor (training_samps x 1)\n",
    "        return self.y_hat * (1 - self.y_hat)\n",
    "\n",
    "    def dl_wrt_q0(self, y_true): #return vextor (training_samps x size of layer 2)\n",
    "        #print(f'this is grad wrt_q0 doe the sconf layetr: {(self.grad_mse(y_true) * self.grad_sigmoid() * self.o1).shape}')\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid() * self.o1\n",
    "\n",
    "    def dl_wrt_b2(self, y_true):\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid()\n",
    "\n",
    "    def dl_wrt_b1(self, y_true):\n",
    "        # print((np.sum(self.grad_mse(y_true), axis=0)).shape)\n",
    "        # print((np.sum(self.grad_sigmoid(), axis=0)).shape)\n",
    "        # print(self.l2.shape)\n",
    "        # print(np.sum((self.grad_relu()), axis=0).reshape(self.l2.shape).shape,'\\n')\n",
    "        \n",
    "        return np.sum(self.grad_mse(y_true), axis=0) * np.sum(self.grad_sigmoid(), axis=0) * self.l2 * np.sum((self.grad_relu()), axis=0).reshape(self.l2.shape)\n",
    "\n",
    "    def grad_wrt_w(self, y_true):\n",
    "        #(1 x training_samps) is the dl_wrt_b1(y_true).T then we multiply accordily by the X whis is (training_samps, 7) to ge full chainrule\n",
    "        #if we did dl_wrt_b1(y_true).T * one specific row we would get the gradiet for each weight in layer one for one training exmple \n",
    "        #so the do for all and them summ across the colomns or jut the dpr product\n",
    "        print(f'{self.grad_mse(y_true).shape} ')\n",
    "        print(self.grad_sigmoid().shape)\n",
    "        print(self.l2.shape)\n",
    "        print(self.grad_relu().shape)\n",
    "\n",
    "        dl_wrt_wi = self.grad_mse(y_true) * self.grad_sigmoid() * self.l2.T * self.grad_relu()\n",
    "        return np.dot(dl_wrt_wi.T, self.X)\n",
    "\n",
    "    def backprop(self, alpha, y_true):\n",
    "        # print(f'l1 before this : {self.l1},new gradit l1: {self.grad_wrt_w(y_true)}')\n",
    "        # print(f'l2 before this : {self.l2},new gradit l2: {alpha * self.dl_wrt_q0(y_true)}')\n",
    "        # print(f'b1 before this : {self.b1},new gradit l1: {alpha * self.dl_wrt_b1(y_true)}')\n",
    "        # print(f'b2 before this : {self.b2},new gradit l1: {alpha * self.dl_wrt_b2(y_true)}\\n\\n')\n",
    "\n",
    "        self.b2 -= alpha * np.sum(self.dl_wrt_b2(y_true), axis=0)\n",
    "        self.l2 -= alpha * np.sum(self.dl_wrt_q0(y_true),axis=0).reshape(self.l2.shape)\n",
    "        self.b1 = self.b1 - (alpha * self.dl_wrt_b1(y_true)).T\n",
    "        #self.b1 -= alpha * self.dl_wrt_b1(y_true)\n",
    "        self.l1 -= alpha * self.grad_wrt_w(y_true).reshape(self.l1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 7\n",
    "hidden_sizes = [3]\n",
    "out_size = 1\n",
    "\n",
    "custom_nn = NN(input_size, hidden_sizes, out_size)\n",
    "\n",
    "class NN_torch(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, out_size):\n",
    "        super(NN_torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.l2 = torch.nn.Linear(hidden_sizes[0], out_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.l1(X)\n",
    "        h1 = self.relu(o1)\n",
    "        o2 = self.l2(h1)\n",
    "        y_hat = self.sigmoid(o2)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_torch(input_size, hidden_sizes, out_size)\n",
    "with torch.no_grad():\n",
    "    model.l1.weight = torch.nn.Parameter(torch.tensor(custom_nn.l1.T, dtype=torch.float32))\n",
    "    model.l1.bias = torch.nn.Parameter(torch.tensor(custom_nn.b1.flatten(), dtype=torch.float32))\n",
    "    model.l2.weight = torch.nn.Parameter(torch.tensor(custom_nn.l2.T, dtype=torch.float32))\n",
    "    model.l2.bias = torch.nn.Parameter(torch.tensor(custom_nn.b2.flatten(), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 :\n",
      " [[0.78594153 0.29394232 0.21848912]\n",
      " [0.61006718 0.20106184 0.17582819]\n",
      " [0.48008921 0.27407688 0.56593221]\n",
      " [0.79211208 0.14628843 0.14534152]\n",
      " [0.56111222 0.56400843 0.62073934]\n",
      " [0.52517558 0.23783087 0.35430834]\n",
      " [0.61155796 0.78962477 0.71271374]]\n",
      "torch l1:\n",
      " [[0.78594154 0.2939423  0.21848911]\n",
      " [0.6100672  0.20106184 0.17582819]\n",
      " [0.48008922 0.27407688 0.5659322 ]\n",
      " [0.79211205 0.14628844 0.14534152]\n",
      " [0.5611122  0.5640084  0.62073934]\n",
      " [0.5251756  0.23783088 0.35430834]\n",
      " [0.61155796 0.78962475 0.7127137 ]]\n",
      "b1 :\n",
      " [[0. 0. 0.]]\n",
      "torch b1:\n",
      " [0. 0. 0.]\n",
      "l2 :\n",
      " [[0.30311333]\n",
      " [0.29498333]\n",
      " [0.36033728]]\n",
      "torch l2 :\n",
      " [[0.30311334]\n",
      " [0.29498333]\n",
      " [0.3603373 ]]\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameters are set correctly\n",
    "print(\"l1 :\\n\", custom_nn.l1)\n",
    "print(\"torch l1:\\n\", model.l1.weight.data.numpy().T)\n",
    "\n",
    "print(\"b1 :\\n\", custom_nn.b1)\n",
    "print(\"torch b1:\\n\", model.l1.bias.data.numpy())\n",
    "\n",
    "print(\"l2 :\\n\", custom_nn.l2)\n",
    "print(\"torch l2 :\\n\", model.l2.weight.data.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_torch = model.forward(torch.Tensor(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits_custom = custom_nn.forward(X_train)\n",
    "#logits_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 1) \n",
      "(393, 1)\n",
      "(3, 1)\n",
      "(393, 3)\n",
      "\n",
      "\n",
      " Gradients for troucbh nn:\n",
      "l1.weight grad:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n",
      "l1.bias grad:\n",
      "tensor([0., 0., 0.])\n",
      "l2.weight grad:\n",
      "tensor([[0., 0., 0.]])\n",
      "l2.bias grad:\n",
      "tensor([0.])\n",
      "\n",
      "l1 : [[0.78594153 0.29394232 0.21848912]\n",
      " [0.61006718 0.20106184 0.17582819]\n",
      " [0.48008921 0.27407688 0.56593221]\n",
      " [0.79211208 0.14628843 0.14534152]\n",
      " [0.56111222 0.56400843 0.62073934]\n",
      " [0.52517558 0.23783087 0.35430834]\n",
      " [0.61155796 0.78962477 0.71271374]]\n",
      "b1 : [[0. 0. 0.]]\n",
      "l2 : [[0.30311333]\n",
      " [0.29498333]\n",
      " [0.36033728]]\n",
      "b2 : [[0.]]\n",
      "\n",
      "\n",
      " Updated parameters:\n",
      "l1.weight tensor([[0.7859, 0.6101, 0.4801, 0.7921, 0.5611, 0.5252, 0.6116],\n",
      "        [0.2939, 0.2011, 0.2741, 0.1463, 0.5640, 0.2378, 0.7896],\n",
      "        [0.2185, 0.1758, 0.5659, 0.1453, 0.6207, 0.3543, 0.7127]])\n",
      "l1.bias tensor([0., 0., 0.])\n",
      "l2.weight tensor([[0.3031, 0.2950, 0.3603]])\n",
      "l2.bias tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g7/qrp04c4n56q06wny0phpjyrc0000gn/T/ipykernel_20268/503546361.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  loss = criterion(logits_torch, torch.Tensor([y_train]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 393, 1])) that is different to the input size (torch.Size([393, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = 0.001\n",
    "custom_nn.backprop(alpha, np.array(y_train))\n",
    "\n",
    "\n",
    "loss = criterion(logits_torch, torch.Tensor([y_train]))\n",
    "\n",
    "# Perform backpropagation and update weights\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "print(\"\\n\\n Gradients for troucbh nn:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad:\\n{param.grad}\")\n",
    "\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# Verify the parameters are set correctly\n",
    "print(\"\\nl1 :\",custom_nn.l1)\n",
    "print(\"b1 :\", custom_nn.b1)\n",
    "print(\"l2 :\", custom_nn.l2)\n",
    "print(\"b2 :\", custom_nn.b2)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n',\"Updated parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
