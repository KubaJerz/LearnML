{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_sf,beds,bath,price,year_built,sqft,price_per_sqft,elevation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(492, 8)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../sf_vs_nyc_house_data.csv', 'r') as f:\n",
    "    labels = f.readline()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "full_data = np.genfromtxt('../../sf_vs_nyc_house_data.csv', delimiter=',')[1:]\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train shape X: (393, 7) y: (99, 1) \n",
      "  Test shape X: (99, 7)  y: (99, 1) \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data[:,1:], full_data[:,:1], test_size=.2)\n",
    "\n",
    "print(f' Train shape X: {X_train.shape} y: {y_test.shape} \\n  Test shape X: {X_test.shape}  y: {y_test.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "NOW THAT WE HAVE THAT WE CAN START TO MAKE THE NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NN_torch(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(NN_torch, self).__init__()\n",
    "\n",
    "#         self.model = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(7,1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(1,1)\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, out_size) -> None:\n",
    "        self.l1 = np.random.rand(input_size,hidden_sizes[0])\n",
    "        self.b1 = np.zeros((1,hidden_sizes[0]))\n",
    "        self.l2 = np.random.rand(hidden_sizes[0],out_size)\n",
    "        self.b2 = np.zeros((1,out_size))\n",
    "\n",
    "    def activation_func(self, o, type):\n",
    "        if type.lower()== 'relu':\n",
    "            return np.where(o >= 0, o, 0)\n",
    "        if type.lower() == 'softmax':\n",
    "            #make val stable\n",
    "            o_stable = o - np.max(o, axis=1, keepdims=True)\n",
    "            o_exp = np.exp(o_stable) #e^x for each x in 0_stable\n",
    "            return o_exp / np.sum(o_exp, axis=1, keepdims=True)\n",
    "        if type.lower() == 'sigmoid':\n",
    "            return 1/(1+np.exp(-o))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.o1 = X @ self.l1 + self.b1\n",
    "        self.h1 = self.activation_func(self.o1, 'relu')\n",
    "        self.o2 = self.h1 @ self.l2 + self.b2\n",
    "        self.y_hat = self.activation_func(self.o2, 'sigmoid')\n",
    "        return self.y_hat\n",
    "    \n",
    "    def calc_loss(self, y_true, y_hat, type):\n",
    "        if type.lower() == 'mse':\n",
    "            return np.sum((y_true - y_hat) ** 2) / len(y_true)\n",
    "\n",
    "    def grad_relu(self, idx=None):\n",
    "        if idx is None:\n",
    "            return (self.h1 > 0).astype(float) #the as type flaot turn the bool mask in 0 or 1 which is what we wnat for the relu derivative\n",
    "        else:\n",
    "            return (self.h1[idx] > 0).astype(float)\n",
    "\n",
    "    def grad_mse(self, y_true): #return vextor (training_samps x 1)\n",
    "        return 2 * (self.y_hat - y_true) / len(y_true)\n",
    "\n",
    "    def grad_sigmoid(self): #return vextor (training_samps x 1)\n",
    "        return self.y_hat * (1 - self.y_hat)\n",
    "\n",
    "    def dl_wrt_q0(self, y_true): #return vextor (training_samps x size of layer 2)\n",
    "        #print(f'this is grad wrt_q0 doe the sconf layetr: {(self.grad_mse(y_true) * self.grad_sigmoid() * self.o1).shape}')\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid() * self.o1\n",
    "\n",
    "    def dl_wrt_b2(self, y_true):\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid()\n",
    "\n",
    "    def dl_wrt_b1(self, y_true):\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid() * self.l2 * self.grad_relu()\n",
    "\n",
    "    def grad_wrt_w(self, y_true):\n",
    "        #(1 x training_samps) is the dl_wrt_b1(y_true).T then we multiply accordily by the X whis is (training_samps, 7) to ge full chainrule\n",
    "        #if we did dl_wrt_b1(y_true).T * one specific row we would get the gradiet for each weight in layer one for one training exmple \n",
    "        #so the do for all and them summ across the colomns or jut the dpr product\n",
    "        return np.dot(self.dl_wrt_b1(y_true).T, self.X)\n",
    "\n",
    "    def backprop(self, alpha, y_true):\n",
    "        # print(f'l1 before this : {self.l1},new gradit l1: {self.grad_wrt_w(y_true)}')\n",
    "        # print(f'l2 before this : {self.l2},new gradit l2: {alpha * self.dl_wrt_q0(y_true)}')\n",
    "        # print(f'b1 before this : {self.b1},new gradit l1: {alpha * self.dl_wrt_b1(y_true)}')\n",
    "        # print(f'b2 before this : {self.b2},new gradit l1: {alpha * self.dl_wrt_b2(y_true)}\\n\\n')\n",
    "\n",
    "        self.b2 -= alpha * np.sum(self.dl_wrt_b2(y_true), axis=0)\n",
    "        self.l2 -= alpha * np.sum(self.dl_wrt_q0(y_true), axis=0)\n",
    "        self.b1 -= alpha * np.sum(self.dl_wrt_b1(y_true), axis=0)\n",
    "        self.l1 -= alpha * self.grad_wrt_w(y_true).reshape(self.l1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 7\n",
    "hidden_sizes = [1]\n",
    "out_size = 1\n",
    "custom_nn = NN(7, [1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_torch(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, out_size):\n",
    "        super(NN_torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.l2 = torch.nn.Linear(hidden_sizes[0], out_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.l1(X)\n",
    "        h1 = self.relu(o1)\n",
    "        o2 = self.l2(h1)\n",
    "        y_hat = self.sigmoid(o2)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_torch(input_size, hidden_sizes, out_size)\n",
    "with torch.no_grad():\n",
    "    model.l1.weight = torch.nn.Parameter(torch.tensor(custom_nn.l1.T, dtype=torch.float32))\n",
    "    model.l1.bias = torch.nn.Parameter(torch.tensor(custom_nn.b1.flatten(), dtype=torch.float32))\n",
    "    model.l2.weight = torch.nn.Parameter(torch.tensor(custom_nn.l2.T, dtype=torch.float32))\n",
    "    model.l2.bias = torch.nn.Parameter(torch.tensor(custom_nn.b2.flatten(), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 :\n",
      " [[0.50778164]\n",
      " [0.4426363 ]\n",
      " [0.31200533]\n",
      " [0.07770886]\n",
      " [0.7090041 ]\n",
      " [0.51672924]\n",
      " [0.89778082]]\n",
      "torch l1:\n",
      " [[0.5077816 ]\n",
      " [0.4426363 ]\n",
      " [0.31200534]\n",
      " [0.07770886]\n",
      " [0.7090041 ]\n",
      " [0.51672924]\n",
      " [0.89778084]]\n",
      "b1 :\n",
      " [[0.]]\n",
      "torch b1:\n",
      " [0.]\n",
      "l2 :\n",
      " [[0.77104113]]\n",
      "torch l2 :\n",
      " [[0.77104115]]\n"
     ]
    }
   ],
   "source": [
    "# Verify the parameters are set correctly\n",
    "print(\"l1 :\\n\", custom_nn.l1)\n",
    "print(\"torch l1:\\n\", model.l1.weight.data.numpy().T)\n",
    "\n",
    "print(\"b1 :\\n\", custom_nn.b1)\n",
    "print(\"torch b1:\\n\", model.l1.bias.data.numpy())\n",
    "\n",
    "print(\"l2 :\\n\", custom_nn.l2)\n",
    "print(\"torch l2 :\\n\", model.l2.weight.data.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_torch = model.forward(torch.Tensor(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits_custom = custom_nn.forward(X_train)\n",
    "#logits_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "\n",
      " Gradients for troucbh nn:\n",
      "l1.weight grad:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0.]])\n",
      "l1.bias grad:\n",
      "tensor([0.])\n",
      "l2.weight grad:\n",
      "tensor([[0.]])\n",
      "l2.bias grad:\n",
      "tensor([0.])\n",
      "\n",
      "l1 : [[0.50778164]\n",
      " [0.4426363 ]\n",
      " [0.31200533]\n",
      " [0.07770886]\n",
      " [0.7090041 ]\n",
      " [0.51672924]\n",
      " [0.89778082]]\n",
      "b1 : [[0.]]\n",
      "l2 : [[0.77104113]]\n",
      "b2 : [[0.]]\n",
      "\n",
      "\n",
      " Updated parameters:\n",
      "l1.weight tensor([[0.5078, 0.4426, 0.3120, 0.0777, 0.7090, 0.5167, 0.8978]])\n",
      "l1.bias tensor([0.])\n",
      "l2.weight tensor([[0.7710]])\n",
      "l2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = 0.001\n",
    "custom_nn.backprop(alpha, np.array(y_train))\n",
    "\n",
    "\n",
    "loss = criterion(logits_torch, torch.Tensor([y_train]))\n",
    "\n",
    "# Perform backpropagation and update weights\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "print(\"\\n\\n Gradients for troucbh nn:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad:\\n{param.grad}\")\n",
    "\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# Verify the parameters are set correctly\n",
    "print(\"\\nl1 :\",custom_nn.l1)\n",
    "print(\"b1 :\", custom_nn.b1)\n",
    "print(\"l2 :\", custom_nn.l2)\n",
    "print(\"b2 :\", custom_nn.b2)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n',\"Updated parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
