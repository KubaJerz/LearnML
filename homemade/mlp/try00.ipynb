{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_sf,beds,bath,price,year_built,sqft,price_per_sqft,elevation\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(492, 8)"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../sf_vs_nyc_house_data.csv', 'r') as f:\n",
    "    labels = f.readline()\n",
    "\n",
    "print(labels)\n",
    "\n",
    "full_data = np.genfromtxt('../../sf_vs_nyc_house_data.csv', delimiter=',')[1:]\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train shape X: (393, 7) y: (99, 1) \n",
      "  Test shape X: (99, 7)  y: (99, 1) \n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_data[:,1:], full_data[:,:1], test_size=.2)\n",
    "\n",
    "print(f' Train shape X: {X_train.shape} y: {y_test.shape} \\n  Test shape X: {X_test.shape}  y: {y_test.shape} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "NOW THAT WE HAVE THAT WE CAN START TO MAKE THE NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, out_size) -> None:\n",
    "        self.l1 = np.random.rand(input_size,hidden_sizes[0])\n",
    "        self.b1 = np.zeros((1,hidden_sizes[0]))\n",
    "        self.l2 = np.random.rand(hidden_sizes[0],out_size)\n",
    "        self.b2 = np.zeros((1,out_size))\n",
    "\n",
    "    def activation_func(self, o, type):\n",
    "        if type.lower()== 'relu':\n",
    "            return np.where(o >= 0, o, 0)\n",
    "        if type.lower() == 'softmax':\n",
    "            #make val stable\n",
    "            o_stable = o - np.max(o, axis=1, keepdims=True)\n",
    "            o_exp = np.exp(o_stable) #e^x for each x in 0_stable\n",
    "            return o_exp / np.sum(o_exp, axis=1, keepdims=True)\n",
    "        if type.lower() == 'sigmoid':\n",
    "            return 1/(1+np.exp(-o))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.o1 = X @ self.l1 + self.b1\n",
    "        self.h1 = self.activation_func(self.o1, 'relu')\n",
    "        self.o2 = self.h1 @ self.l2 + self.b2\n",
    "        self.y_hat = self.activation_func(self.o2, 'sigmoid')\n",
    "        return self.y_hat\n",
    "    \n",
    "    def calc_loss(self, y_true, y_hat, type):\n",
    "        if type.lower() == 'mse':\n",
    "            return np.sum((y_true - y_hat) ** 2) / len(y_true)\n",
    "\n",
    "    def grad_relu(self, idx=None):\n",
    "        if idx is None:\n",
    "            return (self.h1 > 0).astype(float) #the as type flaot turn the bool mask in 0 or 1 which is what we wnat for the relu derivative\n",
    "        else:\n",
    "            return (self.h1[idx] > 0).astype(float)\n",
    "\n",
    "    def grad_mse(self, y_true): #return vextor (training_samps x 1)\n",
    "        return 2 * (self.y_hat - y_true) / len(y_true)\n",
    "    \n",
    "    def grad_sigmoid(self): #return vextor (training_samps x 1)\n",
    "        return self.y_hat * (1 - self.y_hat)\n",
    "\n",
    "    def dl_wrt_q0(self, y_true): #return vextor (training_samps x size of layer 2)\n",
    "        #print(f'this is grad wrt_q0 doe the sconf layetr: {(self.grad_mse(y_true) * self.grad_sigmoid() * self.o1).shape}')\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid() * self.o1\n",
    "\n",
    "    def dl_wrt_b2(self, y_true):\n",
    "        return self.grad_mse(y_true) * self.grad_sigmoid()\n",
    "\n",
    "    def dl_wrt_b1(self, y_true):\n",
    "        return np.sum(self.grad_mse(y_true), axis=0) * np.sum(self.grad_sigmoid(), axis=0) * self.l2 * np.sum((self.grad_relu()), axis=0).reshape(self.l2.shape)\n",
    "\n",
    "    def grad_wrt_w(self, y_true):\n",
    "        #(1 x training_samps) is the dl_wrt_b1(y_true).T then we multiply accordily by the X whis is (training_samps, 7) to ge full chainrule\n",
    "        #if we did dl_wrt_b1(y_true).T * one specific row we would get the gradiet for each weight in layer one for one training exmple \n",
    "        #so the do for all and them summ across the colomns or jut the dpr product\n",
    "        dl_wrt_wi = self.grad_mse(y_true) * self.grad_sigmoid() * self.l2.T * self.grad_relu()\n",
    "        return np.dot(dl_wrt_wi.T, self.X)\n",
    "\n",
    "    def backprop(self, alpha, y_true, print_grads=False):\n",
    "        if print_grads:\n",
    "            print(f'l1 before this : {self.l1},new gradit l1: {self.grad_wrt_w(y_true)}')\n",
    "            print(f'l2 before this : {self.l2},new gradit l2: {alpha * self.dl_wrt_q0(y_true)}')\n",
    "            print(f'b1 before this : {self.b1},new gradit l1: {alpha * self.dl_wrt_b1(y_true)}')\n",
    "            print(f'b2 before this : {self.b2},new gradit l1: {alpha * self.dl_wrt_b2(y_true)}\\n\\n')\n",
    "\n",
    "        self.b2 -= alpha * np.sum(self.dl_wrt_b2(y_true), axis=0)\n",
    "        self.l2 -= alpha * np.sum(self.dl_wrt_q0(y_true),axis=0).reshape(self.l2.shape)\n",
    "        self.b1 = self.b1 - (alpha * self.dl_wrt_b1(y_true)).T\n",
    "        self.l1 -= alpha * self.grad_wrt_w(y_true).reshape(self.l1.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 7\n",
    "hidden_sizes = [3]\n",
    "out_size = 1\n",
    "\n",
    "custom_nn = NN(input_size, hidden_sizes, out_size)\n",
    "\n",
    "class NN_torch(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, out_size):\n",
    "        super(NN_torch, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.l2 = torch.nn.Linear(hidden_sizes[0], out_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o1 = self.l1(X)\n",
    "        h1 = self.relu(o1)\n",
    "        o2 = self.l2(h1)\n",
    "        y_hat = self.sigmoid(o2)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsut setting the weights in the tourch model to be the same as the custom model\n",
    "\n",
    "model = NN_torch(input_size, hidden_sizes, out_size)\n",
    "with torch.no_grad():\n",
    "    model.l1.weight = torch.nn.Parameter(torch.tensor(custom_nn.l1.T, dtype=torch.float32))\n",
    "    model.l1.bias = torch.nn.Parameter(torch.tensor(custom_nn.b1.flatten(), dtype=torch.float32))\n",
    "    model.l2.weight = torch.nn.Parameter(torch.tensor(custom_nn.l2.T, dtype=torch.float32))\n",
    "    model.l2.bias = torch.nn.Parameter(torch.tensor(custom_nn.b2.flatten(), dtype=torch.float32))\n",
    "\n",
    "# Verify the parameters are set correctly\n",
    "print(\"l1 :\\n\", custom_nn.l1)\n",
    "print(\"torch l1:\\n\", model.l1.weight.data.numpy().T)\n",
    "\n",
    "print(\"b1 :\\n\", custom_nn.b1)\n",
    "print(\"torch b1:\\n\", model.l1.bias.data.numpy())\n",
    "\n",
    "print(\"l2 :\\n\", custom_nn.l2)\n",
    "print(\"torch l2 :\\n\", model.l2.weight.data.numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_torch = model.forward(torch.Tensor(X_train))\n",
    "\n",
    "y_hat_custom = custom_nn.forward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 1) \n",
      "(393, 1)\n",
      "(3, 1)\n",
      "(393, 3)\n",
      "\n",
      "\n",
      " Gradients for troucbh nn:\n",
      "l1.weight grad:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n",
      "l1.bias grad:\n",
      "tensor([0., 0., 0.])\n",
      "l2.weight grad:\n",
      "tensor([[0., 0., 0.]])\n",
      "l2.bias grad:\n",
      "tensor([0.])\n",
      "\n",
      "l1 : [[0.63494395 0.00445932 0.19554465]\n",
      " [0.9073893  0.41103075 0.61074752]\n",
      " [0.81401209 0.27966931 0.48087986]\n",
      " [0.84364883 0.78970556 0.86067185]\n",
      " [0.16796443 0.50251376 0.95858351]\n",
      " [0.31962584 0.03270336 0.31038375]\n",
      " [0.06840694 0.18415315 0.4375599 ]]\n",
      "b1 : [[0. 0. 0.]]\n",
      "l2 : [[0.96387474]\n",
      " [0.09893308]\n",
      " [0.33752933]]\n",
      "b2 : [[0.]]\n",
      "\n",
      "\n",
      " Updated parameters:\n",
      "l1.weight tensor([[0.6349, 0.9074, 0.8140, 0.8436, 0.1680, 0.3196, 0.0684],\n",
      "        [0.0045, 0.4110, 0.2797, 0.7897, 0.5025, 0.0327, 0.1842],\n",
      "        [0.1955, 0.6107, 0.4809, 0.8607, 0.9586, 0.3104, 0.4376]])\n",
      "l1.bias tensor([0., 0., 0.])\n",
      "l2.weight tensor([[0.9639, 0.0989, 0.3375]])\n",
      "l2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "custom_nn.backprop(alpha, np.array(y_train)) #custom backprop\n",
    "\n",
    "loss = criterion(y_hat_torch, torch.Tensor([y_train])) #torch backprop\n",
    "\n",
    "#backprop and update weights\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "print(\"\\n\\n Gradients for troucbh nn:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad:\\n{param.grad}\")\n",
    "optimizer.step()\n",
    "\n",
    "# print the new custom params\n",
    "print(\"\\nl1 :\",custom_nn.l1)\n",
    "print(\"b1 :\", custom_nn.b1)\n",
    "print(\"l2 :\", custom_nn.l2)\n",
    "print(\"b2 :\", custom_nn.b2)\n",
    "\n",
    "\n",
    "#see if torch new params matches with the custom new params\n",
    "print('\\n\\n',\"Updated parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
