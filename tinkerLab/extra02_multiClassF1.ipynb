{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Quick Dip ‚ö°\n",
    "\n",
    "Here we are looking at the **inside** of the `multiclass F1 score` in PyTorch.  \n",
    "Specifically, the **'macro'** score üß†.\n",
    "\n",
    "### üéØ Macro F1:\n",
    "> Calculate metrics for each class separately and return their unweighted mean.  \n",
    "> Classes with 0 true and predicted instances are ignored üö´.\n",
    "\n",
    "I **want** the result to be given as a **log string** in Python,  \n",
    "but the string is the **raw markdown code** üìú.\n",
    "\n",
    "They claim that the function will **ignore non-present classes**,  \n",
    "however, we are about to **check** here üïµÔ∏è‚Äç‚ôÇÔ∏è.\n",
    "\n",
    "We are checking because **PyTorch** likes to throw these warnings ‚ö†Ô∏è:\n",
    "```WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.```\n",
    "\n",
    "\n",
    "And if we cast the F1 score to zero and still counted the class,  \n",
    "that would royally **mess up** the scores! üò± So we will **double-check** here üîç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6667, 0.7500])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2,0,0])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8056)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1,0,0])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2,0,0])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we remove all the 0 instances here, then the **macro F1** should be:\n",
    "\n",
    "$$\n",
    "\\frac{(0.75 + 0.6667)}{2} = 0.70833\n",
    "$$\n",
    "\n",
    "and not:\n",
    "\n",
    "$$\n",
    "\\frac{(0.75 + 0.6667 + 0)}{3} = 0.4722\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7083)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor([2,2,2,2,1,1,1])\n",
    "pred = torch.tensor([2,2,2,1,1,1,2])\n",
    "\n",
    "multiclass_f1_score(pred, target, num_classes=3, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it works properly!! WHOOP WHOOP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
